[2023-12-29 01:48:02,666] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: monthly_stocks_etl.etl_process manual__2023-12-29T01:48:01.215151+00:00 [queued]>
[2023-12-29 01:48:02,672] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: monthly_stocks_etl.etl_process manual__2023-12-29T01:48:01.215151+00:00 [queued]>
[2023-12-29 01:48:02,672] {taskinstance.py:1376} INFO - 
--------------------------------------------------------------------------------
[2023-12-29 01:48:02,672] {taskinstance.py:1377} INFO - Starting attempt 1 of 2
[2023-12-29 01:48:02,672] {taskinstance.py:1378} INFO - 
--------------------------------------------------------------------------------
[2023-12-29 01:48:02,680] {taskinstance.py:1397} INFO - Executing <Task(PythonOperator): etl_process> on 2023-12-29 01:48:01.215151+00:00
[2023-12-29 01:48:02,683] {standard_task_runner.py:52} INFO - Started process 188 to run task
[2023-12-29 01:48:02,686] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'monthly_stocks_etl', 'etl_process', 'manual__2023-12-29T01:48:01.215151+00:00', '--job-id', '56', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp8pru3sya', '--error-file', '/tmp/tmpdurfcmk7']
[2023-12-29 01:48:02,687] {standard_task_runner.py:80} INFO - Job 56: Subtask etl_process
[2023-12-29 01:48:02,722] {task_command.py:371} INFO - Running <TaskInstance: monthly_stocks_etl.etl_process manual__2023-12-29T01:48:01.215151+00:00 [running]> on host 476b9ce47c04
[2023-12-29 01:48:02,761] {taskinstance.py:1591} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=monthly_stocks_etl
AIRFLOW_CTX_TASK_ID=etl_process
AIRFLOW_CTX_EXECUTION_DATE=2023-12-29T01:48:01.215151+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=manual__2023-12-29T01:48:01.215151+00:00
[2023-12-29 01:48:03,922] {logging_mixin.py:115} INFO - Connected to Redshift successfully!
[2023-12-29 01:48:07,589] {logging_mixin.py:115} INFO - <Response [200]>
[2023-12-29 01:48:08,487] {logging_mixin.py:115} INFO - <Response [200]>
[2023-12-29 01:48:08,566] {logging_mixin.py:115} INFO - <class 'pandas.core.frame.DataFrame'>
RangeIndex: 578 entries, 0 to 577
Data columns (total 9 columns):
 #   Column           Non-Null Count  Dtype         
---  ------           --------------  -----         
 0   open             578 non-null    float64       
 1   high             578 non-null    float64       
 2   low              578 non-null    float64       
 3   close            578 non-null    float64       
 4   adjusted close   578 non-null    float64       
 5   volume           578 non-null    int64         
 6   dividend amount  578 non-null    float64       
 7   date             578 non-null    datetime64[ns]
 8   symbol           578 non-null    object        
dtypes: datetime64[ns](1), float64(6), int64(1), object(1)
memory usage: 40.8+ KB
[2023-12-29 01:48:09,173] {logging_mixin.py:115} INFO - Tabla creada exitosamente
[2023-12-29 01:48:09,175] {logging_mixin.py:115} INFO - ['open', 'high', 'low', 'close', 'adjusted close', 'volume', 'dividend amount', 'date', 'symbol']
[2023-12-29 01:48:09,177] {logging_mixin.py:115} INFO - 
            CREATE TABLE IF NOT EXISTS monthly_stocks_over_time ("open" FLOAT, "high" FLOAT, "low" FLOAT, "close" FLOAT, "adjusted close" FLOAT, "volume" VARCHAR(255), "dividend amount" FLOAT, "date" TIMESTAMP, "symbol" VARCHAR(255));
            
[2023-12-29 01:48:10,851] {logging_mixin.py:115} INFO - Proceso terminado
[2023-12-29 01:48:10,872] {xcom.py:586} ERROR - Could not serialize the XCom value into JSON. If you are using pickle instead of JSON for XCom, then you need to enable pickle support for XCom in your *** config.
[2023-12-29 01:48:10,874] {taskinstance.py:1909} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 171, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 189, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/etl_dag.py", line 225, in run_etl_process
    kwargs['ti'].xcom_push(key='high_volume_dates', value=high_volume_dates)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 71, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 2412, in xcom_push
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 68, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/xcom.py", line 198, in set
    map_index=map_index,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/xcom.py", line 583, in serialize_value
    return json.dumps(value).encode('UTF-8')
  File "/usr/local/lib/python3.7/json/__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "/usr/local/lib/python3.7/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/usr/local/lib/python3.7/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/usr/local/lib/python3.7/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type Timestamp is not JSON serializable
[2023-12-29 01:48:10,888] {taskinstance.py:1420} INFO - Marking task as UP_FOR_RETRY. dag_id=monthly_stocks_etl, task_id=etl_process, execution_date=20231229T014801, start_date=20231229T014802, end_date=20231229T014810
[2023-12-29 01:48:10,902] {standard_task_runner.py:97} ERROR - Failed to execute job 56 for task etl_process (Object of type Timestamp is not JSON serializable; 188)
[2023-12-29 01:48:10,948] {local_task_job.py:156} INFO - Task exited with return code 1
[2023-12-29 01:48:10,970] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
